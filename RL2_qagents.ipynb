{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Code, Part 2: Agents that act\n",
    "\n",
    "\n",
    "Here we are going to implement a RL agent interacting with a simple environment. In this case, our agent would be a simulated RL mouse, and the environment a 2D maze.\n",
    "\n",
    "## Defining the environment\n",
    "\n",
    "To define the environment, we need to define the set of possible states $S = {s_1, s_2 ... s_N}$, the transition function $P_{s,s'}^{a}$, and the reward transition function $R_{s,s'}^{a}$.\n",
    "\n",
    "In our case, the environment just consists of a 4x4 grid. Our hypothetical agent perceives only one cell at any time---the cell where it is.  Therefore, our states correspond to the sixteen position of the maze, which we can indicate with the coordinates $(0, 0), (0, 1)... (0, 3), (1, 0) ... (3, 3)$.\n",
    "\n",
    "An environment is characterized by two functions:\n",
    "\n",
    "* The state transition probability function $P(s, a, s')$, which the probability of transitioning to a possible state $s'$ when action $a$ is applied during state $s$; and\n",
    "\n",
    "* The reward transition probability function $R(s, a, s')$, which is the probability of receiving a reward $r$ when action $a$ is applied to state $s$ and the environment moves to state $s'\n",
    "\n",
    "In our simple cases, both $P(s,a,s')$ and $R(s,a,s')$ will be simplified to deterministic functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class Maze():\n",
    "    \"\"\"A maze environment\"\"\"\n",
    "\n",
    "    ACTIONS = (\"up\", \"down\", \"left\", \"right\") # List of actions\n",
    "    INITIAL_STATE = (0, 0) # Always starts at the topleft corner\n",
    "    \n",
    "    def __init__(self, fname = \"grid.txt\"):\n",
    "        \"\"\"Inits a maze by loading the grid file\"\"\"\n",
    "        self.grid = np.loadtxt(fname)\n",
    "        self.state = self.INITIAL_STATE\n",
    "        self.end = False\n",
    "\n",
    "\n",
    "    def state_transition(self, state1, action1):\n",
    "        \"Defines the next state gien the \"\n",
    "        s = state1\n",
    "        state2 = copy(state1)\n",
    "        \n",
    "        if action1 in self.ACTIONS:\n",
    "            if action1 == \"up\":\n",
    "                if s[0] > 0:\n",
    "                    state2 = (s[0] - 1, s[1])\n",
    "            \n",
    "            elif action1 == \"left\":\n",
    "                if s[1] > 0:\n",
    "                    state2 = (s[0], s[1] - 1)\n",
    "            \n",
    "            elif action1 == \"down\":\n",
    "                if s[0] < (self.grid.shape[0] - 1):\n",
    "                    state2 = (s[0] + 1, s[1])\n",
    "\n",
    "            elif action1 == \"right\":\n",
    "                if s[1] < (self.grid.shape[1] - 1):\n",
    "                    state2 = (s[0], s[1] + 1)\n",
    "                    \n",
    "        return state2\n",
    "                    \n",
    "    \n",
    "    def reward_transition(self, state1, action1, state2):\n",
    "        \"\"\"Reward is -1 for bouncing against the walls, and whatever is on the grid otherwise\"\"\"\n",
    "        if state1 == state2:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.grid[state2[0], state2[1]]\n",
    "        \n",
    "    \n",
    "    # Quick way to combine State transitions and Reward transitions \n",
    "    def transition(self, action1):\n",
    "        \"\"\"Changes the state following an action\"\"\"\n",
    "        state1 = self.state\n",
    "        state2 = self.state_transition(state1, action1)\n",
    "        reward2 = self.reward_transition(state1, action1, state2)\n",
    "        \n",
    "        self.state = state2\n",
    "        return (state2, reward2) # Returns s_t+1, r_t+1\n",
    "\n",
    "    \n",
    "    def print_state(self):\n",
    "        \"Prints a text representation of the maze (with the agent position)\"\n",
    "        bar = \"-\" * ( 4 * self.grid.shape[1] + 1)\n",
    "        for i in range(self.grid.shape[0]):\n",
    "            row = \"|\"\n",
    "            for j in range(self.grid.shape[1]):\n",
    "                cell = \" \"\n",
    "                if i == self.state[0] and j == self.state[1]:\n",
    "                    cell = \"*\"\n",
    "                row += (\" %s |\" % cell)\n",
    "            print(bar)\n",
    "            print(row)\n",
    "        print(bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the maze\n",
    "The maze is simple but functional. It is easy to create a maze, check the available actions, apply a few actions, and so on. Let's check that our environment works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "| * |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "State after illegal action: (0, 0)\n",
      "Reward after illegal action: -1\n",
      "State after legal action: (1, 0)\n",
      "Reward after legal action: 0.0\n"
     ]
    }
   ],
   "source": [
    "m = Maze()\n",
    "m.print_state()\n",
    "state_after_bad_action = m.state_transition(m.state, \"up\")  # Illigal action: bounces!\n",
    "state_after_good_action = m.state_transition(m.state, \"down\") # Legal action: Goes down\n",
    "\n",
    "print(\"State after illegal action: %s\" % (state_after_bad_action,))\n",
    "print(\"Reward after illegal action: %s\" % (m.reward_transition(m.state, \"up\", state_after_bad_action)))\n",
    "\n",
    "print(\"State after legal action: %s\" % (state_after_good_action,))\n",
    "print(\"Reward after legal action: %s\" % (m.reward_transition(m.state, \"down\", state_after_good_action)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we can easily navigate in our virtual maze by executing the appropriate actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 0), 0.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.transition(\"down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the ```Maze``` object returns two values at the end of each action execution, the new state $s_{t+1}$ and the associated reward $r_{t+1}$. If the ```down``` action was executed with the original maze layout of the ```grid.txt``` file, case, the two values are $s_{t+1} = $ ```(3, 0)``` and $r_{t+1} = $ ```0.0```. We can also execute more actions, and see what happens after a few movements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   | * |   |   |\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "m.transition(\"down\")\n",
    "m.transition(\"down\")\n",
    "m.transition(\"right\")\n",
    "m.print_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Q-Agent\n",
    "Now we can create our own very fantastic agents! As an example, we will create a $Q$-learning agent that interacts with the ``` Maze``` world. The $Q$-table should contain $16 \\times 4 = 64$ different state-action pairs; rather than filling it up right away, we will fill it up as we encounter new state-action pairs, assuming that any previously unencountered state has a value of $Q = 0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, actions=Maze.ACTIONS, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
    "        \"\"\"Creates a Q-agent\"\"\"\n",
    "        self.Q = {}    ## Initial dictionary of (s, a) pairs. At the beginning, it's emtpy.\n",
    "\n",
    "        self.epsilon = epsilon     # Epsilon for e-greey policy\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Temporal discounting\n",
    "        self.actions = actions     # Set of possible actions (provide those of Maze.ACTIONS)\n",
    "\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"Selects an action with a epsilon-greedy policy\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.Q[(state, a)] if (state, a) in self.Q.keys() else 0.0 for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def learnQ(self, state1, action1, reward2, state2, action2):\n",
    "        \"\"\"Updates the Q-values when given an (s,a) pair, the reward value and a new state\"\"\"\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        \n",
    "        q1 = 0.0\n",
    "        \n",
    "        if (state1, action1) in self.Q.keys():\n",
    "            q1 = self.Q[(state1, action1)]\n",
    "        \n",
    "        max_q2 = max([self.Q[(state2, a)] if (state2, a) in self.Q.keys() else 0.0 for a in self.actions])\n",
    "        \n",
    "        rpe = reward2 + g * max_q2 - q1\n",
    "        #print(\"s1 = %s, a1 = %s, s2 = %s, q1 = %.4f, r2 = %.4f, q2 = %.4f\" % (state1, action1, state2, q1, reward2, max_q2))\n",
    "        q1 += a * rpe\n",
    "        #print(\"new q1 = %.4f\" % (q1,))\n",
    "        self.Q[(state1, action1)] = q1\n",
    "\n",
    "            \n",
    "    def calculateV(self, task):\n",
    "        \"\"\"Returns a representation of the best Q values in every states\"\"\"\n",
    "        v = np.zeros(task.grid.shape)\n",
    "        for i in range(v.shape[0]):\n",
    "            for j in range(v.shape[1]):\n",
    "                maxq = max([self.getQ((i, j), a) for a in self.actions])\n",
    "                v[i,j] =maxq\n",
    "        return v\n",
    "    \n",
    "    def visualizeQ(self):\n",
    "        \"\"\"Visualizes the Q tables, one per action\"\"\"\n",
    "        for a in self.actions:\n",
    "            # Create the corresponding state table\n",
    "            mat = np.zeros((4,4))\n",
    "            states = [x for x in self.Q.keys() if x[1] == a]\n",
    "            \n",
    "            for s in states:\n",
    "                row, col = s[0]\n",
    "                mat[row, col] = self.Q[s]\n",
    "            \n",
    "            # Show the Q-table as a heatmap\n",
    "            plt.imshow(mat, interpolation = \"none\", vmin=-1, vmax=10, cmap='jet')\n",
    "            plt.title(\"Q-Table for action '%s'\" % a.upper())\n",
    "            plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction between Environment and Agent\n",
    "Now, two functions to run multiple trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(environment, agent):\n",
    "    \"\"\"A trial ends when the agent gets a reward. The history is returned\"\"\"\n",
    "    state1 = environment.state\n",
    "    action1 = agent.policy(state1)\n",
    "    reward2 = 0.0\n",
    "    history = [state1]\n",
    "    while reward2 != 10:\n",
    "        state2, reward2 = environment.transition(action1)\n",
    "        action2 = agent.policy(state2)\n",
    "        \n",
    "        # Save the states visited\n",
    "        history.append(state2)\n",
    "        \n",
    "        # Update the Q-values for state1, action1\n",
    "        agent.learnQ(state1, action1, reward2, state2, action2)\n",
    "        \n",
    "        state1 = state2\n",
    "        action1 = action2\n",
    "        \n",
    "    return history\n",
    "\n",
    "    \n",
    "def run_trials(environment, agent, n, collect=True):\n",
    "    \"\"\"Runs N trials\"\"\"\n",
    "    history = []\n",
    "    for j in range(n):\n",
    "        h = run_trial(environment, agent)\n",
    "        history += h\n",
    "        environment.state = Maze.INITIAL_STATE\n",
    "    \n",
    "    return history\n",
    "        \n",
    "def visualize_history(history):\n",
    "    mat = np.zeros((4,4))            \n",
    "    for s in history:\n",
    "        row, col = s\n",
    "        mat[row, col] += 1\n",
    "    \n",
    "    mat /= np.sum(mat)\n",
    "    mat *= 5  # min number of moves\n",
    "    \n",
    "    plt.imshow(mat, interpolation = \"none\", vmin=0, vmax=1, cmap='jet')\n",
    "    plt.title(\"Preferred path\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the agent\n",
    "\n",
    "Here a few tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\na.visualizeQ()\\nh = run_trials(m, a, 10)\\nvisualize_history(h) \\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Maze()\n",
    "a = Agent()\n",
    "run_trials(m, a, 100)\n",
    "\"\"\" \n",
    "a.visualizeQ()\n",
    "h = run_trials(m, a, 10)\n",
    "visualize_history(h) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are the $Q$ values changing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials_for_altair(environment, agent, n, collect=True):\n",
    "    \"\"\"Runs N trials\"\"\"\n",
    "    state_action = {} \n",
    "    # init all state_actions\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for direction in ['down', 'right', 'up', 'left']:\n",
    "                state_action[str(((i, j), direction))] = [0] \n",
    "\n",
    "    for j in range(n):\n",
    "        run_trial(environment, agent)\n",
    "        all_keys = set(state_action.keys())\n",
    "        # keys with new values\n",
    "        for key, val in agent.Q.items():\n",
    "            state_action[str(key)].append(val)\n",
    "            all_keys.remove(str(key))\n",
    "        # keys without new values\n",
    "        for key in all_keys:\n",
    "            state_action[str(key)].append(state_action[str(key)][-1])\n",
    "        environment.state = Maze.INITIAL_STATE\n",
    "        \n",
    "    import pandas as pd\n",
    "    location = []\n",
    "    run = []\n",
    "    q_value = []\n",
    "    for loc in state_action.keys():\n",
    "        for i in range(len(state_action[loc])):\n",
    "            location.append(loc)\n",
    "            run.append(i)\n",
    "            q_value.append(state_action[loc][i])\n",
    "    df = pd.DataFrame({\"location\":location, \"run\":run, \"q_value\":q_value})\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Maze()\n",
    "a = Agent()\n",
    "df = run_trials_for_altair(m, a, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import altair as alt\\n\\nslider = alt.binding_range(min=1, max=100, step=1)\\nselect_run = alt.selection_single(name=\"iteration\", fields=[\\'run\\'], bind=slider)\\nalt.data_transformers.enable(\\'default\\', max_rows=None)\\nalt.Chart(df).mark_bar().encode(\\n    x=\\'location:N\\',\\n    y=alt.Y(\\'q_value:Q\\', scale=alt.Scale(domain=(0, 11))),\\n).add_selection(\\n    select_run\\n).transform_filter(\\n    select_run\\n) \\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import altair as alt\n",
    "\n",
    "slider = alt.binding_range(min=1, max=100, step=1)\n",
    "select_run = alt.selection_single(name=\"iteration\", fields=['run'], bind=slider)\n",
    "alt.data_transformers.enable('default', max_rows=None)\n",
    "alt.Chart(df).mark_bar().encode(\n",
    "    x='location:N',\n",
    "    y=alt.Y('q_value:Q', scale=alt.Scale(domain=(0, 11))),\n",
    ").add_selection(\n",
    "    select_run\n",
    ").transform_filter(\n",
    "    select_run\n",
    ") \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 0), 'right'): 7.289999999999978,\n",
       " ((0, 1), 'right'): 6.5609999999959,\n",
       " ((0, 2), 'right'): 4.32089817354012,\n",
       " ((0, 3), 'down'): 7.378391291451503,\n",
       " ((1, 3), 'up'): 2.363675481007641,\n",
       " ((1, 3), 'right'): 4.522485512218539,\n",
       " ((1, 3), 'down'): 8.999999999986725,\n",
       " ((2, 3), 'down'): 1.415015914802821,\n",
       " ((3, 3), 'left'): 0.0,\n",
       " ((3, 2), 'down'): -0.1,\n",
       " ((3, 2), 'right'): 0.0,\n",
       " ((3, 3), 'down'): -0.1,\n",
       " ((3, 3), 'up'): 4.979038469034337,\n",
       " ((2, 3), 'left'): 9.99999999999926,\n",
       " ((0, 1), 'left'): 6.560999999986223,\n",
       " ((0, 0), 'up'): 5.560999999999394,\n",
       " ((0, 0), 'left'): 5.56099999996289,\n",
       " ((0, 1), 'down'): 8.09999999999998,\n",
       " ((1, 1), 'left'): 7.289999999995676,\n",
       " ((1, 0), 'left'): 3.5393145144608336,\n",
       " ((1, 0), 'down'): 1.7206266156320065,\n",
       " ((2, 0), 'down'): 0.011200238900002544,\n",
       " ((3, 0), 'down'): -0.19,\n",
       " ((3, 0), 'left'): -0.1,\n",
       " ((3, 0), 'right'): 0.5392430571338386,\n",
       " ((3, 1), 'right'): 0.0,\n",
       " ((3, 3), 'right'): -0.1,\n",
       " ((0, 0), 'down'): 7.28999999990845,\n",
       " ((1, 0), 'up'): 5.060053600166008,\n",
       " ((1, 0), 'right'): 8.09999999999998,\n",
       " ((1, 1), 'down'): 7.2899999999952865,\n",
       " ((2, 1), 'down'): 1.6741723593789304,\n",
       " ((3, 1), 'up'): 5.001406681338098,\n",
       " ((2, 1), 'up'): 8.099999999999731,\n",
       " ((1, 1), 'right'): 8.999999999999986,\n",
       " ((1, 2), 'left'): 8.099999999997099,\n",
       " ((1, 2), 'down'): 9.999999999999993,\n",
       " ((0, 2), 'left'): 7.289999999999978,\n",
       " ((0, 1), 'up'): 6.289999999997528,\n",
       " ((0, 2), 'up'): 3.6024017332560194,\n",
       " ((2, 0), 'up'): 5.4057424662620965,\n",
       " ((2, 0), 'left'): -0.1,\n",
       " ((3, 0), 'up'): 0.0,\n",
       " ((2, 0), 'right'): 0.0,\n",
       " ((1, 2), 'up'): 6.560999999939286,\n",
       " ((0, 3), 'right'): -0.1310257443281629,\n",
       " ((0, 3), 'up'): -0.19,\n",
       " ((0, 3), 'left'): 0.0,\n",
       " ((1, 1), 'up'): 7.289999999999939,\n",
       " ((1, 2), 'right'): 8.099999999729633,\n",
       " ((2, 3), 'right'): 2.797783089289002,\n",
       " ((2, 1), 'left'): 1.6867709874284005,\n",
       " ((2, 3), 'up'): 5.745406473406765,\n",
       " ((1, 3), 'left'): 6.712320754503896,\n",
       " ((0, 2), 'down'): 7.146979811148155,\n",
       " ((2, 1), 'right'): 3.439}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
